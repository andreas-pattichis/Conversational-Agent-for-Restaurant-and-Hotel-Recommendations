{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.chdir(\"../..\")\n",
    "import slot_extraction.utilities.slots as slt\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "\n",
    "filtered = slt.filter_dataset(dataset['test'])\n",
    "slot_data = slt.construct_slot_extraction_data(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results where obtained using the Dataset ground truth and the same evaluation metrics as in the evaluation notebook, but removing the question mark slots as we considered they were part of the third task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions results **with** similarity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrange dom_slot is:  with value:  star\n",
      "Extrange dom_slot is:  with value: :30\n",
      "Extrange dom_slot is:  with value:  oriental\n",
      "Extrange dom_slot is:  with value:  hotel\n",
      "Extrange dom_slot is:  with value:  guest house\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8734560708459567, 0.9099295945617868, 0.8913198573127228)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_data['predictions'] = slot_data['utterance'].apply(slt.predict_ner)\n",
    "\n",
    "slt.get_evaluation_metrics(slot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the results obtained without taking into account the question mark slots is the following and using the similarity classifier for normalization are the following:\n",
    "\n",
    "- Precision: 87.3%\n",
    "- Recall: 90.9%\n",
    "- F1-score: 89.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction results **without** similarity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8179911442554183, 0.8521485797523671, 0.8347205707491081)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slot_data['predictions2'] = slot_data['utterance'].apply(slt.predict_ner, args=(False,))\n",
    "slt.get_evaluation_metrics(slot_data, predicition_column='predictions2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the results obtained without taking into account the question mark slots is the following and **without** the similarity classifier for normalization are the following:\n",
    "\n",
    "- Precision: 81.8%\n",
    "- Recall: 85.2%\n",
    "- F1-score: 83.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
